{
 "metadata": {
  "name": "",
  "signature": "sha256:754ee90770b0cd21df2dbca58be2c373db8c4cfb764b2664a9a072896d5437ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the tutorials on DeepLearning.net [here](http://www.deeplearning.net/tutorial/gettingstarted.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd .."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/dorislee/Desktop/DNN\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "import cPickle, gzip, numpy\n",
      "# Load the dataset\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use Theano shared variables in GPU computing to prevent large overhead when data-copying. Each minibatch is a slice of the data represent by a single variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import *\n",
      "import theano.tensor as T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shared_dataset(data_xy):\n",
      "    \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "    The reason we store our dataset in shared variables is to allow\n",
      "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "    is needed (the default behaviour if the data is not in a shared\n",
      "    variable) would lead to a large decrease in performance.\n",
      "    \"\"\"\n",
      "    data_x, data_y = data_xy\n",
      "    shared_x = theano.shared(numpy.asarray(data_x, dtype=theano.config.floatX))\n",
      "    shared_y = theano.shared(numpy.asarray(data_y, dtype=theano.config.floatX))\n",
      "    # When storing data on the GPU it has to be stored as floats\n",
      "    # therefore we will store the labels as ``floatX`` as well\n",
      "    # (``shared_y`` does exactly that). But during our computations\n",
      "    # we need them as ints (we use labels as index, and if they are\n",
      "    # floats it doesn't make sense) therefore instead of returning\n",
      "    # ``shared_y`` we will have to cast it to int. This little hack\n",
      "    # lets us get around this issue\n",
      "    return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "test_set_x, test_set_y = shared_dataset(test_set)\n",
      "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
      "train_set_x, train_set_y = shared_dataset(train_set)\n",
      "\n",
      "batch_size = 500    # size of the minibatch\n",
      "\n",
      "# accessing the third minibatch of the training set\n",
      "\n",
      "data  = train_set_x[2 * 500: 3 * 500]\n",
      "label = train_set_y[2 * 500: 3 * 500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Learning a Classifier "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- minimize error (zero-one loss $l_{0,1}$) on unseen examples\n",
      "- f(x) = classification function = $argmax_kP(Y=k|x,\\theta)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.scalar(\"x\")\n",
      "y = x**2\n",
      "p_y_given_x = theano.function([x],y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zero_one_loss = T.sum(T.neq(T.argmax(p_y_given_x),y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Maximize log-likelihood = minimize Negative  log-likelihood (NLL)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NLL = -T.sum(T.log(p_y_given_x)[T.arange(y.shape[0]), y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Stochastic Gradient descent(SGD) same as GD but faster $\\because$ eval only on a few example and not the whole training set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}